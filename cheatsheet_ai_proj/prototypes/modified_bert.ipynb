{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 6207, 7222, 23804, 3336, 1026, 16129, 1028, 102], [101, 6207, 11345, 7222, 23804, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['apple pineapple baby <html>', 'apple pie pineapple'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1026, 1044, 2487, 2806, 1027, 13138, 1028, 6207, 1026, 1013, 1044, 2487, 1028, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('<h1 style=123>apple</h1>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method decode in module transformers.tokenization_utils_base:\n",
      "\n",
      "decode(token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str method of transformers.models.bert.tokenization_bert.BertTokenizer instance\n",
      "    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      "    tokens and clean up tokenization spaces.\n",
      "    \n",
      "    Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      "    \n",
      "    Args:\n",
      "        token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      "            List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      "        skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to remove special tokens in the decoding.\n",
      "        clean_up_tokenization_spaces (`bool`, *optional*):\n",
      "            Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
      "            `self.clean_up_tokenization_spaces`.\n",
      "        kwargs (additional keyword arguments, *optional*):\n",
      "            Will be passed to the underlying model specific decode method.\n",
      "    \n",
      "    Returns:\n",
      "        `str`: The decoded sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 11:40:23.109267: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-10 11:40:23.109310: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-10 11:40:23.109999: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-10 11:40:23.171104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-10 11:40:24.152215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 7592, 102], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sp/Downloads/mindmap_ai/modified_bert.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids=input_ids,\n",
    "token_type_ids=token_type_ids,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [ 0.3739, -0.0156, -0.2456,  ..., -0.0317,  0.5514, -0.5241],\n",
       "         [-0.4815, -0.0189,  0.0092,  ..., -0.2806,  0.3895, -0.2815]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import LongTensor\n",
    "\n",
    "\n",
    "model.embeddings(**{'input_ids': LongTensor([[101, 7592, 102]]), 'token_type_ids': LongTensor([[0, 0, 0]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_output = model.embeddings(**{'input_ids': LongTensor([[101, 7592, 102]]), 'token_type_ids': LongTensor([[0, 0, 0]])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sp/Downloads/mindmap_ai/modified_bert.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m embedding_output\u001b[39m.\u001b[39mshape\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m (batch_size, max_seq_len, vec_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_output.shape\n",
    "(batch_size, max_seq_len, vec_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LongTensor([[1, 1, 1]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sp/Downloads/mindmap_ai/modified_bert.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     embedding_output,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mLongTensor([[\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], [\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m]])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    611\u001b[0m         hidden_states,\n\u001b[1;32m    612\u001b[0m         attention_mask,\n\u001b[1;32m    613\u001b[0m         layer_head_mask,\n\u001b[1;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    616\u001b[0m         past_key_value,\n\u001b[1;32m    617\u001b[0m         output_attentions,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    496\u001b[0m         hidden_states,\n\u001b[1;32m    497\u001b[0m         attention_mask,\n\u001b[1;32m    498\u001b[0m         head_mask,\n\u001b[1;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    426\u001b[0m         hidden_states,\n\u001b[1;32m    427\u001b[0m         attention_mask,\n\u001b[1;32m    428\u001b[0m         head_mask,\n\u001b[1;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    431\u001b[0m         past_key_value,\n\u001b[1;32m    432\u001b[0m         output_attentions,\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:350\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    347\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[0;32m--> 350\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39;49m attention_mask\n\u001b[1;32m    352\u001b[0m \u001b[39m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m attention_probs \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attention_scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "model.encoder(\n",
    "    embedding_output,\n",
    "    attention_mask=LongTensor([[1, 1, 1], [3, 3, 3]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_char_lens = []\n",
    "for token in  [101, 1026, 1044, 2487, 2806, 1027, 13138, 1028, 6207, 1026, 1013, 1044, 2487, 1028, 102]:\n",
    "    string_token = tokenizer.decode([token], skip_special_tokens=True)\n",
    "    if string_token[:2] =='##':\n",
    "        string_token = string_token[2:]\n",
    "    if string_token[-2:] =='##':\n",
    "        string_token = string_token[:2]\n",
    "    token_char_lens.append(len(string_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 5, 1, 3, 1, 5, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_char_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = nn.Linear(768, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2001,  0.4336,  0.5887,  ..., -0.3227, -0.1087,  0.2730],\n",
       "         [ 0.0098,  0.0414, -0.1186,  ...,  0.0267, -0.6299, -0.0955],\n",
       "         [-0.4080,  0.2878,  0.0338,  ..., -0.3930,  0.0195,  0.1860]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apple(embedding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def compute_token_char_length(tokens: List[int]) -> List[int]:\n",
    "    token_char_lens = []\n",
    "    for token in  tokens:\n",
    "        string_token = tokenizer.decode([token], skip_special_tokens=True)\n",
    "        if string_token[:2] =='##':\n",
    "            string_token = string_token[2:]\n",
    "        if string_token[-2:] =='##':\n",
    "            string_token = string_token[:2]\n",
    "        token_char_lens.append(len(string_token))\n",
    "    return token_char_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor\n",
    "import torch\n",
    "import math\n",
    "\n",
    "def positional_encoding(positions: FloatTensor, d_model=50):\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    \n",
    "    pe = torch.zeros(positions.shape[0], d_model)\n",
    "    positions = positions.unsqueeze(-1)\n",
    "    pe[:, 0::2] = torch.sin(positions * div_term)\n",
    "    pe[:, 1::2] = torch.cos(positions * div_term)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "positional_encoding() missing 1 required positional argument: 'positions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#X24sdW50aXRsZWQ%3D?line=0'>1</a>\u001b[0m positional_encoding()\n",
      "\u001b[0;31mTypeError\u001b[0m: positional_encoding() missing 1 required positional argument: 'positions'"
     ]
    }
   ],
   "source": [
    "positional_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Dropout, ReLU\n",
    "\n",
    "class ModifiedBert(nn.Module):\n",
    "    def __init__(self):\n",
    "        self.model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        self.dropout = Dropout()\n",
    "        self.relu = ReLU()\n",
    "        \n",
    "    def forward(self, input_ids: LongTensor, attention_mask: LongTensor):\n",
    "        embedding_output = model.embeddings(input_ids=input_ids)\n",
    "        augmented_input = self.linear(embedding_output)\n",
    "        augmented_input = self.relu(augmented_input)\n",
    "        augmented_input = self.dropout(augmented_input)\n",
    "        out = model.encoder(augmented_input, attention_mask=attention_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "attention = nn.MultiheadAttention(768, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sp/Downloads/mindmap_ai/modified_bert.ipynb Cell 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/sp/Downloads/mindmap_ai/modified_bert.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m attention(torch\u001b[39m.\u001b[39mFloatTensor([[i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m768\u001b[39m)]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m]))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: MultiheadAttention.forward() missing 2 required positional arguments: 'key' and 'value'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "attention(torch.FloatTensor([[i for i in range(768)]*3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import LongTensor\n",
    "a = LongTensor([[1,2,3,4,5], [1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [1, 2]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f901336c430>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import LongTensor\n",
    "import torch\n",
    "d = TensorDataset(LongTensor([i for i in range(89*512*768)]).reshape((89, 512, 768)))\n",
    "DataLoader(d, sampler=None, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[     0,      1,      2,  ...,    765,    766,    767],\n",
       "         [   768,    769,    770,  ...,   1533,   1534,   1535],\n",
       "         [  1536,   1537,   1538,  ...,   2301,   2302,   2303],\n",
       "         ...,\n",
       "         [390912, 390913, 390914,  ..., 391677, 391678, 391679],\n",
       "         [391680, 391681, 391682,  ..., 392445, 392446, 392447],\n",
       "         [392448, 392449, 392450,  ..., 393213, 393214, 393215]]),)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[     0,      1,      2,  ...,    765,    766,    767],\n",
       "          [   768,    769,    770,  ...,   1533,   1534,   1535],\n",
       "          [  1536,   1537,   1538,  ...,   2301,   2302,   2303],\n",
       "          ...,\n",
       "          [390912, 390913, 390914,  ..., 391677, 391678, 391679],\n",
       "          [391680, 391681, 391682,  ..., 392445, 392446, 392447],\n",
       "          [392448, 392449, 392450,  ..., 393213, 393214, 393215]]),),\n",
       " (tensor([[393216, 393217, 393218,  ..., 393981, 393982, 393983],\n",
       "          [393984, 393985, 393986,  ..., 394749, 394750, 394751],\n",
       "          [394752, 394753, 394754,  ..., 395517, 395518, 395519],\n",
       "          ...,\n",
       "          [784128, 784129, 784130,  ..., 784893, 784894, 784895],\n",
       "          [784896, 784897, 784898,  ..., 785661, 785662, 785663],\n",
       "          [785664, 785665, 785666,  ..., 786429, 786430, 786431]]),),\n",
       " (tensor([[ 786432,  786433,  786434,  ...,  787197,  787198,  787199],\n",
       "          [ 787200,  787201,  787202,  ...,  787965,  787966,  787967],\n",
       "          [ 787968,  787969,  787970,  ...,  788733,  788734,  788735],\n",
       "          ...,\n",
       "          [1177344, 1177345, 1177346,  ..., 1178109, 1178110, 1178111],\n",
       "          [1178112, 1178113, 1178114,  ..., 1178877, 1178878, 1178879],\n",
       "          [1178880, 1178881, 1178882,  ..., 1179645, 1179646, 1179647]]),),\n",
       " (tensor([[1179648, 1179649, 1179650,  ..., 1180413, 1180414, 1180415],\n",
       "          [1180416, 1180417, 1180418,  ..., 1181181, 1181182, 1181183],\n",
       "          [1181184, 1181185, 1181186,  ..., 1181949, 1181950, 1181951],\n",
       "          ...,\n",
       "          [1570560, 1570561, 1570562,  ..., 1571325, 1571326, 1571327],\n",
       "          [1571328, 1571329, 1571330,  ..., 1572093, 1572094, 1572095],\n",
       "          [1572096, 1572097, 1572098,  ..., 1572861, 1572862, 1572863]]),),\n",
       " (tensor([[1572864, 1572865, 1572866,  ..., 1573629, 1573630, 1573631],\n",
       "          [1573632, 1573633, 1573634,  ..., 1574397, 1574398, 1574399],\n",
       "          [1574400, 1574401, 1574402,  ..., 1575165, 1575166, 1575167],\n",
       "          ...,\n",
       "          [1963776, 1963777, 1963778,  ..., 1964541, 1964542, 1964543],\n",
       "          [1964544, 1964545, 1964546,  ..., 1965309, 1965310, 1965311],\n",
       "          [1965312, 1965313, 1965314,  ..., 1966077, 1966078, 1966079]]),),\n",
       " (tensor([[1966080, 1966081, 1966082,  ..., 1966845, 1966846, 1966847],\n",
       "          [1966848, 1966849, 1966850,  ..., 1967613, 1967614, 1967615],\n",
       "          [1967616, 1967617, 1967618,  ..., 1968381, 1968382, 1968383],\n",
       "          ...,\n",
       "          [2356992, 2356993, 2356994,  ..., 2357757, 2357758, 2357759],\n",
       "          [2357760, 2357761, 2357762,  ..., 2358525, 2358526, 2358527],\n",
       "          [2358528, 2358529, 2358530,  ..., 2359293, 2359294, 2359295]]),),\n",
       " (tensor([[2359296, 2359297, 2359298,  ..., 2360061, 2360062, 2360063],\n",
       "          [2360064, 2360065, 2360066,  ..., 2360829, 2360830, 2360831],\n",
       "          [2360832, 2360833, 2360834,  ..., 2361597, 2361598, 2361599],\n",
       "          ...,\n",
       "          [2750208, 2750209, 2750210,  ..., 2750973, 2750974, 2750975],\n",
       "          [2750976, 2750977, 2750978,  ..., 2751741, 2751742, 2751743],\n",
       "          [2751744, 2751745, 2751746,  ..., 2752509, 2752510, 2752511]]),),\n",
       " (tensor([[2752512, 2752513, 2752514,  ..., 2753277, 2753278, 2753279],\n",
       "          [2753280, 2753281, 2753282,  ..., 2754045, 2754046, 2754047],\n",
       "          [2754048, 2754049, 2754050,  ..., 2754813, 2754814, 2754815],\n",
       "          ...,\n",
       "          [3143424, 3143425, 3143426,  ..., 3144189, 3144190, 3144191],\n",
       "          [3144192, 3144193, 3144194,  ..., 3144957, 3144958, 3144959],\n",
       "          [3144960, 3144961, 3144962,  ..., 3145725, 3145726, 3145727]]),),\n",
       " (tensor([[3145728, 3145729, 3145730,  ..., 3146493, 3146494, 3146495],\n",
       "          [3146496, 3146497, 3146498,  ..., 3147261, 3147262, 3147263],\n",
       "          [3147264, 3147265, 3147266,  ..., 3148029, 3148030, 3148031],\n",
       "          ...,\n",
       "          [3536640, 3536641, 3536642,  ..., 3537405, 3537406, 3537407],\n",
       "          [3537408, 3537409, 3537410,  ..., 3538173, 3538174, 3538175],\n",
       "          [3538176, 3538177, 3538178,  ..., 3538941, 3538942, 3538943]]),),\n",
       " (tensor([[3538944, 3538945, 3538946,  ..., 3539709, 3539710, 3539711],\n",
       "          [3539712, 3539713, 3539714,  ..., 3540477, 3540478, 3540479],\n",
       "          [3540480, 3540481, 3540482,  ..., 3541245, 3541246, 3541247],\n",
       "          ...,\n",
       "          [3929856, 3929857, 3929858,  ..., 3930621, 3930622, 3930623],\n",
       "          [3930624, 3930625, 3930626,  ..., 3931389, 3931390, 3931391],\n",
       "          [3931392, 3931393, 3931394,  ..., 3932157, 3932158, 3932159]]),),\n",
       " (tensor([[3932160, 3932161, 3932162,  ..., 3932925, 3932926, 3932927],\n",
       "          [3932928, 3932929, 3932930,  ..., 3933693, 3933694, 3933695],\n",
       "          [3933696, 3933697, 3933698,  ..., 3934461, 3934462, 3934463],\n",
       "          ...,\n",
       "          [4323072, 4323073, 4323074,  ..., 4323837, 4323838, 4323839],\n",
       "          [4323840, 4323841, 4323842,  ..., 4324605, 4324606, 4324607],\n",
       "          [4324608, 4324609, 4324610,  ..., 4325373, 4325374, 4325375]]),),\n",
       " (tensor([[4325376, 4325377, 4325378,  ..., 4326141, 4326142, 4326143],\n",
       "          [4326144, 4326145, 4326146,  ..., 4326909, 4326910, 4326911],\n",
       "          [4326912, 4326913, 4326914,  ..., 4327677, 4327678, 4327679],\n",
       "          ...,\n",
       "          [4716288, 4716289, 4716290,  ..., 4717053, 4717054, 4717055],\n",
       "          [4717056, 4717057, 4717058,  ..., 4717821, 4717822, 4717823],\n",
       "          [4717824, 4717825, 4717826,  ..., 4718589, 4718590, 4718591]]),),\n",
       " (tensor([[4718592, 4718593, 4718594,  ..., 4719357, 4719358, 4719359],\n",
       "          [4719360, 4719361, 4719362,  ..., 4720125, 4720126, 4720127],\n",
       "          [4720128, 4720129, 4720130,  ..., 4720893, 4720894, 4720895],\n",
       "          ...,\n",
       "          [5109504, 5109505, 5109506,  ..., 5110269, 5110270, 5110271],\n",
       "          [5110272, 5110273, 5110274,  ..., 5111037, 5111038, 5111039],\n",
       "          [5111040, 5111041, 5111042,  ..., 5111805, 5111806, 5111807]]),),\n",
       " (tensor([[5111808, 5111809, 5111810,  ..., 5112573, 5112574, 5112575],\n",
       "          [5112576, 5112577, 5112578,  ..., 5113341, 5113342, 5113343],\n",
       "          [5113344, 5113345, 5113346,  ..., 5114109, 5114110, 5114111],\n",
       "          ...,\n",
       "          [5502720, 5502721, 5502722,  ..., 5503485, 5503486, 5503487],\n",
       "          [5503488, 5503489, 5503490,  ..., 5504253, 5504254, 5504255],\n",
       "          [5504256, 5504257, 5504258,  ..., 5505021, 5505022, 5505023]]),),\n",
       " (tensor([[5505024, 5505025, 5505026,  ..., 5505789, 5505790, 5505791],\n",
       "          [5505792, 5505793, 5505794,  ..., 5506557, 5506558, 5506559],\n",
       "          [5506560, 5506561, 5506562,  ..., 5507325, 5507326, 5507327],\n",
       "          ...,\n",
       "          [5895936, 5895937, 5895938,  ..., 5896701, 5896702, 5896703],\n",
       "          [5896704, 5896705, 5896706,  ..., 5897469, 5897470, 5897471],\n",
       "          [5897472, 5897473, 5897474,  ..., 5898237, 5898238, 5898239]]),),\n",
       " (tensor([[5898240, 5898241, 5898242,  ..., 5899005, 5899006, 5899007],\n",
       "          [5899008, 5899009, 5899010,  ..., 5899773, 5899774, 5899775],\n",
       "          [5899776, 5899777, 5899778,  ..., 5900541, 5900542, 5900543],\n",
       "          ...,\n",
       "          [6289152, 6289153, 6289154,  ..., 6289917, 6289918, 6289919],\n",
       "          [6289920, 6289921, 6289922,  ..., 6290685, 6290686, 6290687],\n",
       "          [6290688, 6290689, 6290690,  ..., 6291453, 6291454, 6291455]]),),\n",
       " (tensor([[6291456, 6291457, 6291458,  ..., 6292221, 6292222, 6292223],\n",
       "          [6292224, 6292225, 6292226,  ..., 6292989, 6292990, 6292991],\n",
       "          [6292992, 6292993, 6292994,  ..., 6293757, 6293758, 6293759],\n",
       "          ...,\n",
       "          [6682368, 6682369, 6682370,  ..., 6683133, 6683134, 6683135],\n",
       "          [6683136, 6683137, 6683138,  ..., 6683901, 6683902, 6683903],\n",
       "          [6683904, 6683905, 6683906,  ..., 6684669, 6684670, 6684671]]),),\n",
       " (tensor([[6684672, 6684673, 6684674,  ..., 6685437, 6685438, 6685439],\n",
       "          [6685440, 6685441, 6685442,  ..., 6686205, 6686206, 6686207],\n",
       "          [6686208, 6686209, 6686210,  ..., 6686973, 6686974, 6686975],\n",
       "          ...,\n",
       "          [7075584, 7075585, 7075586,  ..., 7076349, 7076350, 7076351],\n",
       "          [7076352, 7076353, 7076354,  ..., 7077117, 7077118, 7077119],\n",
       "          [7077120, 7077121, 7077122,  ..., 7077885, 7077886, 7077887]]),),\n",
       " (tensor([[7077888, 7077889, 7077890,  ..., 7078653, 7078654, 7078655],\n",
       "          [7078656, 7078657, 7078658,  ..., 7079421, 7079422, 7079423],\n",
       "          [7079424, 7079425, 7079426,  ..., 7080189, 7080190, 7080191],\n",
       "          ...,\n",
       "          [7468800, 7468801, 7468802,  ..., 7469565, 7469566, 7469567],\n",
       "          [7469568, 7469569, 7469570,  ..., 7470333, 7470334, 7470335],\n",
       "          [7470336, 7470337, 7470338,  ..., 7471101, 7471102, 7471103]]),),\n",
       " (tensor([[7471104, 7471105, 7471106,  ..., 7471869, 7471870, 7471871],\n",
       "          [7471872, 7471873, 7471874,  ..., 7472637, 7472638, 7472639],\n",
       "          [7472640, 7472641, 7472642,  ..., 7473405, 7473406, 7473407],\n",
       "          ...,\n",
       "          [7862016, 7862017, 7862018,  ..., 7862781, 7862782, 7862783],\n",
       "          [7862784, 7862785, 7862786,  ..., 7863549, 7863550, 7863551],\n",
       "          [7863552, 7863553, 7863554,  ..., 7864317, 7864318, 7864319]]),),\n",
       " (tensor([[7864320, 7864321, 7864322,  ..., 7865085, 7865086, 7865087],\n",
       "          [7865088, 7865089, 7865090,  ..., 7865853, 7865854, 7865855],\n",
       "          [7865856, 7865857, 7865858,  ..., 7866621, 7866622, 7866623],\n",
       "          ...,\n",
       "          [8255232, 8255233, 8255234,  ..., 8255997, 8255998, 8255999],\n",
       "          [8256000, 8256001, 8256002,  ..., 8256765, 8256766, 8256767],\n",
       "          [8256768, 8256769, 8256770,  ..., 8257533, 8257534, 8257535]]),),\n",
       " (tensor([[8257536, 8257537, 8257538,  ..., 8258301, 8258302, 8258303],\n",
       "          [8258304, 8258305, 8258306,  ..., 8259069, 8259070, 8259071],\n",
       "          [8259072, 8259073, 8259074,  ..., 8259837, 8259838, 8259839],\n",
       "          ...,\n",
       "          [8648448, 8648449, 8648450,  ..., 8649213, 8649214, 8649215],\n",
       "          [8649216, 8649217, 8649218,  ..., 8649981, 8649982, 8649983],\n",
       "          [8649984, 8649985, 8649986,  ..., 8650749, 8650750, 8650751]]),),\n",
       " (tensor([[8650752, 8650753, 8650754,  ..., 8651517, 8651518, 8651519],\n",
       "          [8651520, 8651521, 8651522,  ..., 8652285, 8652286, 8652287],\n",
       "          [8652288, 8652289, 8652290,  ..., 8653053, 8653054, 8653055],\n",
       "          ...,\n",
       "          [9041664, 9041665, 9041666,  ..., 9042429, 9042430, 9042431],\n",
       "          [9042432, 9042433, 9042434,  ..., 9043197, 9043198, 9043199],\n",
       "          [9043200, 9043201, 9043202,  ..., 9043965, 9043966, 9043967]]),),\n",
       " (tensor([[9043968, 9043969, 9043970,  ..., 9044733, 9044734, 9044735],\n",
       "          [9044736, 9044737, 9044738,  ..., 9045501, 9045502, 9045503],\n",
       "          [9045504, 9045505, 9045506,  ..., 9046269, 9046270, 9046271],\n",
       "          ...,\n",
       "          [9434880, 9434881, 9434882,  ..., 9435645, 9435646, 9435647],\n",
       "          [9435648, 9435649, 9435650,  ..., 9436413, 9436414, 9436415],\n",
       "          [9436416, 9436417, 9436418,  ..., 9437181, 9437182, 9437183]]),),\n",
       " (tensor([[9437184, 9437185, 9437186,  ..., 9437949, 9437950, 9437951],\n",
       "          [9437952, 9437953, 9437954,  ..., 9438717, 9438718, 9438719],\n",
       "          [9438720, 9438721, 9438722,  ..., 9439485, 9439486, 9439487],\n",
       "          ...,\n",
       "          [9828096, 9828097, 9828098,  ..., 9828861, 9828862, 9828863],\n",
       "          [9828864, 9828865, 9828866,  ..., 9829629, 9829630, 9829631],\n",
       "          [9829632, 9829633, 9829634,  ..., 9830397, 9830398, 9830399]]),),\n",
       " (tensor([[ 9830400,  9830401,  9830402,  ...,  9831165,  9831166,  9831167],\n",
       "          [ 9831168,  9831169,  9831170,  ...,  9831933,  9831934,  9831935],\n",
       "          [ 9831936,  9831937,  9831938,  ...,  9832701,  9832702,  9832703],\n",
       "          ...,\n",
       "          [10221312, 10221313, 10221314,  ..., 10222077, 10222078, 10222079],\n",
       "          [10222080, 10222081, 10222082,  ..., 10222845, 10222846, 10222847],\n",
       "          [10222848, 10222849, 10222850,  ..., 10223613, 10223614, 10223615]]),),\n",
       " (tensor([[10223616, 10223617, 10223618,  ..., 10224381, 10224382, 10224383],\n",
       "          [10224384, 10224385, 10224386,  ..., 10225149, 10225150, 10225151],\n",
       "          [10225152, 10225153, 10225154,  ..., 10225917, 10225918, 10225919],\n",
       "          ...,\n",
       "          [10614528, 10614529, 10614530,  ..., 10615293, 10615294, 10615295],\n",
       "          [10615296, 10615297, 10615298,  ..., 10616061, 10616062, 10616063],\n",
       "          [10616064, 10616065, 10616066,  ..., 10616829, 10616830, 10616831]]),),\n",
       " (tensor([[10616832, 10616833, 10616834,  ..., 10617597, 10617598, 10617599],\n",
       "          [10617600, 10617601, 10617602,  ..., 10618365, 10618366, 10618367],\n",
       "          [10618368, 10618369, 10618370,  ..., 10619133, 10619134, 10619135],\n",
       "          ...,\n",
       "          [11007744, 11007745, 11007746,  ..., 11008509, 11008510, 11008511],\n",
       "          [11008512, 11008513, 11008514,  ..., 11009277, 11009278, 11009279],\n",
       "          [11009280, 11009281, 11009282,  ..., 11010045, 11010046, 11010047]]),),\n",
       " (tensor([[11010048, 11010049, 11010050,  ..., 11010813, 11010814, 11010815],\n",
       "          [11010816, 11010817, 11010818,  ..., 11011581, 11011582, 11011583],\n",
       "          [11011584, 11011585, 11011586,  ..., 11012349, 11012350, 11012351],\n",
       "          ...,\n",
       "          [11400960, 11400961, 11400962,  ..., 11401725, 11401726, 11401727],\n",
       "          [11401728, 11401729, 11401730,  ..., 11402493, 11402494, 11402495],\n",
       "          [11402496, 11402497, 11402498,  ..., 11403261, 11403262, 11403263]]),),\n",
       " (tensor([[11403264, 11403265, 11403266,  ..., 11404029, 11404030, 11404031],\n",
       "          [11404032, 11404033, 11404034,  ..., 11404797, 11404798, 11404799],\n",
       "          [11404800, 11404801, 11404802,  ..., 11405565, 11405566, 11405567],\n",
       "          ...,\n",
       "          [11794176, 11794177, 11794178,  ..., 11794941, 11794942, 11794943],\n",
       "          [11794944, 11794945, 11794946,  ..., 11795709, 11795710, 11795711],\n",
       "          [11795712, 11795713, 11795714,  ..., 11796477, 11796478, 11796479]]),),\n",
       " (tensor([[11796480, 11796481, 11796482,  ..., 11797245, 11797246, 11797247],\n",
       "          [11797248, 11797249, 11797250,  ..., 11798013, 11798014, 11798015],\n",
       "          [11798016, 11798017, 11798018,  ..., 11798781, 11798782, 11798783],\n",
       "          ...,\n",
       "          [12187392, 12187393, 12187394,  ..., 12188157, 12188158, 12188159],\n",
       "          [12188160, 12188161, 12188162,  ..., 12188925, 12188926, 12188927],\n",
       "          [12188928, 12188929, 12188930,  ..., 12189693, 12189694, 12189695]]),),\n",
       " (tensor([[12189696, 12189697, 12189698,  ..., 12190461, 12190462, 12190463],\n",
       "          [12190464, 12190465, 12190466,  ..., 12191229, 12191230, 12191231],\n",
       "          [12191232, 12191233, 12191234,  ..., 12191997, 12191998, 12191999],\n",
       "          ...,\n",
       "          [12580608, 12580609, 12580610,  ..., 12581373, 12581374, 12581375],\n",
       "          [12581376, 12581377, 12581378,  ..., 12582141, 12582142, 12582143],\n",
       "          [12582144, 12582145, 12582146,  ..., 12582909, 12582910, 12582911]]),),\n",
       " (tensor([[12582912, 12582913, 12582914,  ..., 12583677, 12583678, 12583679],\n",
       "          [12583680, 12583681, 12583682,  ..., 12584445, 12584446, 12584447],\n",
       "          [12584448, 12584449, 12584450,  ..., 12585213, 12585214, 12585215],\n",
       "          ...,\n",
       "          [12973824, 12973825, 12973826,  ..., 12974589, 12974590, 12974591],\n",
       "          [12974592, 12974593, 12974594,  ..., 12975357, 12975358, 12975359],\n",
       "          [12975360, 12975361, 12975362,  ..., 12976125, 12976126, 12976127]]),),\n",
       " (tensor([[12976128, 12976129, 12976130,  ..., 12976893, 12976894, 12976895],\n",
       "          [12976896, 12976897, 12976898,  ..., 12977661, 12977662, 12977663],\n",
       "          [12977664, 12977665, 12977666,  ..., 12978429, 12978430, 12978431],\n",
       "          ...,\n",
       "          [13367040, 13367041, 13367042,  ..., 13367805, 13367806, 13367807],\n",
       "          [13367808, 13367809, 13367810,  ..., 13368573, 13368574, 13368575],\n",
       "          [13368576, 13368577, 13368578,  ..., 13369341, 13369342, 13369343]]),),\n",
       " (tensor([[13369344, 13369345, 13369346,  ..., 13370109, 13370110, 13370111],\n",
       "          [13370112, 13370113, 13370114,  ..., 13370877, 13370878, 13370879],\n",
       "          [13370880, 13370881, 13370882,  ..., 13371645, 13371646, 13371647],\n",
       "          ...,\n",
       "          [13760256, 13760257, 13760258,  ..., 13761021, 13761022, 13761023],\n",
       "          [13761024, 13761025, 13761026,  ..., 13761789, 13761790, 13761791],\n",
       "          [13761792, 13761793, 13761794,  ..., 13762557, 13762558, 13762559]]),),\n",
       " (tensor([[13762560, 13762561, 13762562,  ..., 13763325, 13763326, 13763327],\n",
       "          [13763328, 13763329, 13763330,  ..., 13764093, 13764094, 13764095],\n",
       "          [13764096, 13764097, 13764098,  ..., 13764861, 13764862, 13764863],\n",
       "          ...,\n",
       "          [14153472, 14153473, 14153474,  ..., 14154237, 14154238, 14154239],\n",
       "          [14154240, 14154241, 14154242,  ..., 14155005, 14155006, 14155007],\n",
       "          [14155008, 14155009, 14155010,  ..., 14155773, 14155774, 14155775]]),),\n",
       " (tensor([[14155776, 14155777, 14155778,  ..., 14156541, 14156542, 14156543],\n",
       "          [14156544, 14156545, 14156546,  ..., 14157309, 14157310, 14157311],\n",
       "          [14157312, 14157313, 14157314,  ..., 14158077, 14158078, 14158079],\n",
       "          ...,\n",
       "          [14546688, 14546689, 14546690,  ..., 14547453, 14547454, 14547455],\n",
       "          [14547456, 14547457, 14547458,  ..., 14548221, 14548222, 14548223],\n",
       "          [14548224, 14548225, 14548226,  ..., 14548989, 14548990, 14548991]]),),\n",
       " (tensor([[14548992, 14548993, 14548994,  ..., 14549757, 14549758, 14549759],\n",
       "          [14549760, 14549761, 14549762,  ..., 14550525, 14550526, 14550527],\n",
       "          [14550528, 14550529, 14550530,  ..., 14551293, 14551294, 14551295],\n",
       "          ...,\n",
       "          [14939904, 14939905, 14939906,  ..., 14940669, 14940670, 14940671],\n",
       "          [14940672, 14940673, 14940674,  ..., 14941437, 14941438, 14941439],\n",
       "          [14941440, 14941441, 14941442,  ..., 14942205, 14942206, 14942207]]),),\n",
       " (tensor([[14942208, 14942209, 14942210,  ..., 14942973, 14942974, 14942975],\n",
       "          [14942976, 14942977, 14942978,  ..., 14943741, 14943742, 14943743],\n",
       "          [14943744, 14943745, 14943746,  ..., 14944509, 14944510, 14944511],\n",
       "          ...,\n",
       "          [15333120, 15333121, 15333122,  ..., 15333885, 15333886, 15333887],\n",
       "          [15333888, 15333889, 15333890,  ..., 15334653, 15334654, 15334655],\n",
       "          [15334656, 15334657, 15334658,  ..., 15335421, 15335422, 15335423]]),),\n",
       " (tensor([[15335424, 15335425, 15335426,  ..., 15336189, 15336190, 15336191],\n",
       "          [15336192, 15336193, 15336194,  ..., 15336957, 15336958, 15336959],\n",
       "          [15336960, 15336961, 15336962,  ..., 15337725, 15337726, 15337727],\n",
       "          ...,\n",
       "          [15726336, 15726337, 15726338,  ..., 15727101, 15727102, 15727103],\n",
       "          [15727104, 15727105, 15727106,  ..., 15727869, 15727870, 15727871],\n",
       "          [15727872, 15727873, 15727874,  ..., 15728637, 15728638, 15728639]]),),\n",
       " (tensor([[15728640, 15728641, 15728642,  ..., 15729405, 15729406, 15729407],\n",
       "          [15729408, 15729409, 15729410,  ..., 15730173, 15730174, 15730175],\n",
       "          [15730176, 15730177, 15730178,  ..., 15730941, 15730942, 15730943],\n",
       "          ...,\n",
       "          [16119552, 16119553, 16119554,  ..., 16120317, 16120318, 16120319],\n",
       "          [16120320, 16120321, 16120322,  ..., 16121085, 16121086, 16121087],\n",
       "          [16121088, 16121089, 16121090,  ..., 16121853, 16121854, 16121855]]),),\n",
       " (tensor([[16121856, 16121857, 16121858,  ..., 16122621, 16122622, 16122623],\n",
       "          [16122624, 16122625, 16122626,  ..., 16123389, 16123390, 16123391],\n",
       "          [16123392, 16123393, 16123394,  ..., 16124157, 16124158, 16124159],\n",
       "          ...,\n",
       "          [16512768, 16512769, 16512770,  ..., 16513533, 16513534, 16513535],\n",
       "          [16513536, 16513537, 16513538,  ..., 16514301, 16514302, 16514303],\n",
       "          [16514304, 16514305, 16514306,  ..., 16515069, 16515070, 16515071]]),),\n",
       " (tensor([[16515072, 16515073, 16515074,  ..., 16515837, 16515838, 16515839],\n",
       "          [16515840, 16515841, 16515842,  ..., 16516605, 16516606, 16516607],\n",
       "          [16516608, 16516609, 16516610,  ..., 16517373, 16517374, 16517375],\n",
       "          ...,\n",
       "          [16905984, 16905985, 16905986,  ..., 16906749, 16906750, 16906751],\n",
       "          [16906752, 16906753, 16906754,  ..., 16907517, 16907518, 16907519],\n",
       "          [16907520, 16907521, 16907522,  ..., 16908285, 16908286, 16908287]]),),\n",
       " (tensor([[16908288, 16908289, 16908290,  ..., 16909053, 16909054, 16909055],\n",
       "          [16909056, 16909057, 16909058,  ..., 16909821, 16909822, 16909823],\n",
       "          [16909824, 16909825, 16909826,  ..., 16910589, 16910590, 16910591],\n",
       "          ...,\n",
       "          [17299200, 17299201, 17299202,  ..., 17299965, 17299966, 17299967],\n",
       "          [17299968, 17299969, 17299970,  ..., 17300733, 17300734, 17300735],\n",
       "          [17300736, 17300737, 17300738,  ..., 17301501, 17301502, 17301503]]),),\n",
       " (tensor([[17301504, 17301505, 17301506,  ..., 17302269, 17302270, 17302271],\n",
       "          [17302272, 17302273, 17302274,  ..., 17303037, 17303038, 17303039],\n",
       "          [17303040, 17303041, 17303042,  ..., 17303805, 17303806, 17303807],\n",
       "          ...,\n",
       "          [17692416, 17692417, 17692418,  ..., 17693181, 17693182, 17693183],\n",
       "          [17693184, 17693185, 17693186,  ..., 17693949, 17693950, 17693951],\n",
       "          [17693952, 17693953, 17693954,  ..., 17694717, 17694718, 17694719]]),),\n",
       " (tensor([[17694720, 17694721, 17694722,  ..., 17695485, 17695486, 17695487],\n",
       "          [17695488, 17695489, 17695490,  ..., 17696253, 17696254, 17696255],\n",
       "          [17696256, 17696257, 17696258,  ..., 17697021, 17697022, 17697023],\n",
       "          ...,\n",
       "          [18085632, 18085633, 18085634,  ..., 18086397, 18086398, 18086399],\n",
       "          [18086400, 18086401, 18086402,  ..., 18087165, 18087166, 18087167],\n",
       "          [18087168, 18087169, 18087170,  ..., 18087933, 18087934, 18087935]]),),\n",
       " (tensor([[18087936, 18087937, 18087938,  ..., 18088701, 18088702, 18088703],\n",
       "          [18088704, 18088705, 18088706,  ..., 18089469, 18089470, 18089471],\n",
       "          [18089472, 18089473, 18089474,  ..., 18090237, 18090238, 18090239],\n",
       "          ...,\n",
       "          [18478848, 18478849, 18478850,  ..., 18479613, 18479614, 18479615],\n",
       "          [18479616, 18479617, 18479618,  ..., 18480381, 18480382, 18480383],\n",
       "          [18480384, 18480385, 18480386,  ..., 18481149, 18481150, 18481151]]),),\n",
       " (tensor([[18481152, 18481153, 18481154,  ..., 18481917, 18481918, 18481919],\n",
       "          [18481920, 18481921, 18481922,  ..., 18482685, 18482686, 18482687],\n",
       "          [18482688, 18482689, 18482690,  ..., 18483453, 18483454, 18483455],\n",
       "          ...,\n",
       "          [18872064, 18872065, 18872066,  ..., 18872829, 18872830, 18872831],\n",
       "          [18872832, 18872833, 18872834,  ..., 18873597, 18873598, 18873599],\n",
       "          [18873600, 18873601, 18873602,  ..., 18874365, 18874366, 18874367]]),),\n",
       " (tensor([[18874368, 18874369, 18874370,  ..., 18875133, 18875134, 18875135],\n",
       "          [18875136, 18875137, 18875138,  ..., 18875901, 18875902, 18875903],\n",
       "          [18875904, 18875905, 18875906,  ..., 18876669, 18876670, 18876671],\n",
       "          ...,\n",
       "          [19265280, 19265281, 19265282,  ..., 19266045, 19266046, 19266047],\n",
       "          [19266048, 19266049, 19266050,  ..., 19266813, 19266814, 19266815],\n",
       "          [19266816, 19266817, 19266818,  ..., 19267581, 19267582, 19267583]]),),\n",
       " (tensor([[19267584, 19267585, 19267586,  ..., 19268349, 19268350, 19268351],\n",
       "          [19268352, 19268353, 19268354,  ..., 19269117, 19269118, 19269119],\n",
       "          [19269120, 19269121, 19269122,  ..., 19269885, 19269886, 19269887],\n",
       "          ...,\n",
       "          [19658496, 19658497, 19658498,  ..., 19659261, 19659262, 19659263],\n",
       "          [19659264, 19659265, 19659266,  ..., 19660029, 19660030, 19660031],\n",
       "          [19660032, 19660033, 19660034,  ..., 19660797, 19660798, 19660799]]),),\n",
       " (tensor([[19660800, 19660801, 19660802,  ..., 19661565, 19661566, 19661567],\n",
       "          [19661568, 19661569, 19661570,  ..., 19662333, 19662334, 19662335],\n",
       "          [19662336, 19662337, 19662338,  ..., 19663101, 19663102, 19663103],\n",
       "          ...,\n",
       "          [20051712, 20051713, 20051714,  ..., 20052477, 20052478, 20052479],\n",
       "          [20052480, 20052481, 20052482,  ..., 20053245, 20053246, 20053247],\n",
       "          [20053248, 20053249, 20053250,  ..., 20054013, 20054014, 20054015]]),),\n",
       " (tensor([[20054016, 20054017, 20054018,  ..., 20054781, 20054782, 20054783],\n",
       "          [20054784, 20054785, 20054786,  ..., 20055549, 20055550, 20055551],\n",
       "          [20055552, 20055553, 20055554,  ..., 20056317, 20056318, 20056319],\n",
       "          ...,\n",
       "          [20444928, 20444929, 20444930,  ..., 20445693, 20445694, 20445695],\n",
       "          [20445696, 20445697, 20445698,  ..., 20446461, 20446462, 20446463],\n",
       "          [20446464, 20446465, 20446466,  ..., 20447229, 20447230, 20447231]]),),\n",
       " (tensor([[20447232, 20447233, 20447234,  ..., 20447997, 20447998, 20447999],\n",
       "          [20448000, 20448001, 20448002,  ..., 20448765, 20448766, 20448767],\n",
       "          [20448768, 20448769, 20448770,  ..., 20449533, 20449534, 20449535],\n",
       "          ...,\n",
       "          [20838144, 20838145, 20838146,  ..., 20838909, 20838910, 20838911],\n",
       "          [20838912, 20838913, 20838914,  ..., 20839677, 20839678, 20839679],\n",
       "          [20839680, 20839681, 20839682,  ..., 20840445, 20840446, 20840447]]),),\n",
       " (tensor([[20840448, 20840449, 20840450,  ..., 20841213, 20841214, 20841215],\n",
       "          [20841216, 20841217, 20841218,  ..., 20841981, 20841982, 20841983],\n",
       "          [20841984, 20841985, 20841986,  ..., 20842749, 20842750, 20842751],\n",
       "          ...,\n",
       "          [21231360, 21231361, 21231362,  ..., 21232125, 21232126, 21232127],\n",
       "          [21232128, 21232129, 21232130,  ..., 21232893, 21232894, 21232895],\n",
       "          [21232896, 21232897, 21232898,  ..., 21233661, 21233662, 21233663]]),),\n",
       " (tensor([[21233664, 21233665, 21233666,  ..., 21234429, 21234430, 21234431],\n",
       "          [21234432, 21234433, 21234434,  ..., 21235197, 21235198, 21235199],\n",
       "          [21235200, 21235201, 21235202,  ..., 21235965, 21235966, 21235967],\n",
       "          ...,\n",
       "          [21624576, 21624577, 21624578,  ..., 21625341, 21625342, 21625343],\n",
       "          [21625344, 21625345, 21625346,  ..., 21626109, 21626110, 21626111],\n",
       "          [21626112, 21626113, 21626114,  ..., 21626877, 21626878, 21626879]]),),\n",
       " (tensor([[21626880, 21626881, 21626882,  ..., 21627645, 21627646, 21627647],\n",
       "          [21627648, 21627649, 21627650,  ..., 21628413, 21628414, 21628415],\n",
       "          [21628416, 21628417, 21628418,  ..., 21629181, 21629182, 21629183],\n",
       "          ...,\n",
       "          [22017792, 22017793, 22017794,  ..., 22018557, 22018558, 22018559],\n",
       "          [22018560, 22018561, 22018562,  ..., 22019325, 22019326, 22019327],\n",
       "          [22019328, 22019329, 22019330,  ..., 22020093, 22020094, 22020095]]),),\n",
       " (tensor([[22020096, 22020097, 22020098,  ..., 22020861, 22020862, 22020863],\n",
       "          [22020864, 22020865, 22020866,  ..., 22021629, 22021630, 22021631],\n",
       "          [22021632, 22021633, 22021634,  ..., 22022397, 22022398, 22022399],\n",
       "          ...,\n",
       "          [22411008, 22411009, 22411010,  ..., 22411773, 22411774, 22411775],\n",
       "          [22411776, 22411777, 22411778,  ..., 22412541, 22412542, 22412543],\n",
       "          [22412544, 22412545, 22412546,  ..., 22413309, 22413310, 22413311]]),),\n",
       " (tensor([[22413312, 22413313, 22413314,  ..., 22414077, 22414078, 22414079],\n",
       "          [22414080, 22414081, 22414082,  ..., 22414845, 22414846, 22414847],\n",
       "          [22414848, 22414849, 22414850,  ..., 22415613, 22415614, 22415615],\n",
       "          ...,\n",
       "          [22804224, 22804225, 22804226,  ..., 22804989, 22804990, 22804991],\n",
       "          [22804992, 22804993, 22804994,  ..., 22805757, 22805758, 22805759],\n",
       "          [22805760, 22805761, 22805762,  ..., 22806525, 22806526, 22806527]]),),\n",
       " (tensor([[22806528, 22806529, 22806530,  ..., 22807293, 22807294, 22807295],\n",
       "          [22807296, 22807297, 22807298,  ..., 22808061, 22808062, 22808063],\n",
       "          [22808064, 22808065, 22808066,  ..., 22808829, 22808830, 22808831],\n",
       "          ...,\n",
       "          [23197440, 23197441, 23197442,  ..., 23198205, 23198206, 23198207],\n",
       "          [23198208, 23198209, 23198210,  ..., 23198973, 23198974, 23198975],\n",
       "          [23198976, 23198977, 23198978,  ..., 23199741, 23199742, 23199743]]),),\n",
       " (tensor([[23199744, 23199745, 23199746,  ..., 23200509, 23200510, 23200511],\n",
       "          [23200512, 23200513, 23200514,  ..., 23201277, 23201278, 23201279],\n",
       "          [23201280, 23201281, 23201282,  ..., 23202045, 23202046, 23202047],\n",
       "          ...,\n",
       "          [23590656, 23590657, 23590658,  ..., 23591421, 23591422, 23591423],\n",
       "          [23591424, 23591425, 23591426,  ..., 23592189, 23592190, 23592191],\n",
       "          [23592192, 23592193, 23592194,  ..., 23592957, 23592958, 23592959]]),),\n",
       " (tensor([[23592960, 23592961, 23592962,  ..., 23593725, 23593726, 23593727],\n",
       "          [23593728, 23593729, 23593730,  ..., 23594493, 23594494, 23594495],\n",
       "          [23594496, 23594497, 23594498,  ..., 23595261, 23595262, 23595263],\n",
       "          ...,\n",
       "          [23983872, 23983873, 23983874,  ..., 23984637, 23984638, 23984639],\n",
       "          [23984640, 23984641, 23984642,  ..., 23985405, 23985406, 23985407],\n",
       "          [23985408, 23985409, 23985410,  ..., 23986173, 23986174, 23986175]]),),\n",
       " (tensor([[23986176, 23986177, 23986178,  ..., 23986941, 23986942, 23986943],\n",
       "          [23986944, 23986945, 23986946,  ..., 23987709, 23987710, 23987711],\n",
       "          [23987712, 23987713, 23987714,  ..., 23988477, 23988478, 23988479],\n",
       "          ...,\n",
       "          [24377088, 24377089, 24377090,  ..., 24377853, 24377854, 24377855],\n",
       "          [24377856, 24377857, 24377858,  ..., 24378621, 24378622, 24378623],\n",
       "          [24378624, 24378625, 24378626,  ..., 24379389, 24379390, 24379391]]),),\n",
       " (tensor([[24379392, 24379393, 24379394,  ..., 24380157, 24380158, 24380159],\n",
       "          [24380160, 24380161, 24380162,  ..., 24380925, 24380926, 24380927],\n",
       "          [24380928, 24380929, 24380930,  ..., 24381693, 24381694, 24381695],\n",
       "          ...,\n",
       "          [24770304, 24770305, 24770306,  ..., 24771069, 24771070, 24771071],\n",
       "          [24771072, 24771073, 24771074,  ..., 24771837, 24771838, 24771839],\n",
       "          [24771840, 24771841, 24771842,  ..., 24772605, 24772606, 24772607]]),),\n",
       " (tensor([[24772608, 24772609, 24772610,  ..., 24773373, 24773374, 24773375],\n",
       "          [24773376, 24773377, 24773378,  ..., 24774141, 24774142, 24774143],\n",
       "          [24774144, 24774145, 24774146,  ..., 24774909, 24774910, 24774911],\n",
       "          ...,\n",
       "          [25163520, 25163521, 25163522,  ..., 25164285, 25164286, 25164287],\n",
       "          [25164288, 25164289, 25164290,  ..., 25165053, 25165054, 25165055],\n",
       "          [25165056, 25165057, 25165058,  ..., 25165821, 25165822, 25165823]]),),\n",
       " (tensor([[25165824, 25165825, 25165826,  ..., 25166589, 25166590, 25166591],\n",
       "          [25166592, 25166593, 25166594,  ..., 25167357, 25167358, 25167359],\n",
       "          [25167360, 25167361, 25167362,  ..., 25168125, 25168126, 25168127],\n",
       "          ...,\n",
       "          [25556736, 25556737, 25556738,  ..., 25557501, 25557502, 25557503],\n",
       "          [25557504, 25557505, 25557506,  ..., 25558269, 25558270, 25558271],\n",
       "          [25558272, 25558273, 25558274,  ..., 25559037, 25559038, 25559039]]),),\n",
       " (tensor([[25559040, 25559041, 25559042,  ..., 25559805, 25559806, 25559807],\n",
       "          [25559808, 25559809, 25559810,  ..., 25560573, 25560574, 25560575],\n",
       "          [25560576, 25560577, 25560578,  ..., 25561341, 25561342, 25561343],\n",
       "          ...,\n",
       "          [25949952, 25949953, 25949954,  ..., 25950717, 25950718, 25950719],\n",
       "          [25950720, 25950721, 25950722,  ..., 25951485, 25951486, 25951487],\n",
       "          [25951488, 25951489, 25951490,  ..., 25952253, 25952254, 25952255]]),),\n",
       " (tensor([[25952256, 25952257, 25952258,  ..., 25953021, 25953022, 25953023],\n",
       "          [25953024, 25953025, 25953026,  ..., 25953789, 25953790, 25953791],\n",
       "          [25953792, 25953793, 25953794,  ..., 25954557, 25954558, 25954559],\n",
       "          ...,\n",
       "          [26343168, 26343169, 26343170,  ..., 26343933, 26343934, 26343935],\n",
       "          [26343936, 26343937, 26343938,  ..., 26344701, 26344702, 26344703],\n",
       "          [26344704, 26344705, 26344706,  ..., 26345469, 26345470, 26345471]]),),\n",
       " (tensor([[26345472, 26345473, 26345474,  ..., 26346237, 26346238, 26346239],\n",
       "          [26346240, 26346241, 26346242,  ..., 26347005, 26347006, 26347007],\n",
       "          [26347008, 26347009, 26347010,  ..., 26347773, 26347774, 26347775],\n",
       "          ...,\n",
       "          [26736384, 26736385, 26736386,  ..., 26737149, 26737150, 26737151],\n",
       "          [26737152, 26737153, 26737154,  ..., 26737917, 26737918, 26737919],\n",
       "          [26737920, 26737921, 26737922,  ..., 26738685, 26738686, 26738687]]),),\n",
       " (tensor([[26738688, 26738689, 26738690,  ..., 26739453, 26739454, 26739455],\n",
       "          [26739456, 26739457, 26739458,  ..., 26740221, 26740222, 26740223],\n",
       "          [26740224, 26740225, 26740226,  ..., 26740989, 26740990, 26740991],\n",
       "          ...,\n",
       "          [27129600, 27129601, 27129602,  ..., 27130365, 27130366, 27130367],\n",
       "          [27130368, 27130369, 27130370,  ..., 27131133, 27131134, 27131135],\n",
       "          [27131136, 27131137, 27131138,  ..., 27131901, 27131902, 27131903]]),),\n",
       " (tensor([[27131904, 27131905, 27131906,  ..., 27132669, 27132670, 27132671],\n",
       "          [27132672, 27132673, 27132674,  ..., 27133437, 27133438, 27133439],\n",
       "          [27133440, 27133441, 27133442,  ..., 27134205, 27134206, 27134207],\n",
       "          ...,\n",
       "          [27522816, 27522817, 27522818,  ..., 27523581, 27523582, 27523583],\n",
       "          [27523584, 27523585, 27523586,  ..., 27524349, 27524350, 27524351],\n",
       "          [27524352, 27524353, 27524354,  ..., 27525117, 27525118, 27525119]]),),\n",
       " (tensor([[27525120, 27525121, 27525122,  ..., 27525885, 27525886, 27525887],\n",
       "          [27525888, 27525889, 27525890,  ..., 27526653, 27526654, 27526655],\n",
       "          [27526656, 27526657, 27526658,  ..., 27527421, 27527422, 27527423],\n",
       "          ...,\n",
       "          [27916032, 27916033, 27916034,  ..., 27916797, 27916798, 27916799],\n",
       "          [27916800, 27916801, 27916802,  ..., 27917565, 27917566, 27917567],\n",
       "          [27917568, 27917569, 27917570,  ..., 27918333, 27918334, 27918335]]),),\n",
       " (tensor([[27918336, 27918337, 27918338,  ..., 27919101, 27919102, 27919103],\n",
       "          [27919104, 27919105, 27919106,  ..., 27919869, 27919870, 27919871],\n",
       "          [27919872, 27919873, 27919874,  ..., 27920637, 27920638, 27920639],\n",
       "          ...,\n",
       "          [28309248, 28309249, 28309250,  ..., 28310013, 28310014, 28310015],\n",
       "          [28310016, 28310017, 28310018,  ..., 28310781, 28310782, 28310783],\n",
       "          [28310784, 28310785, 28310786,  ..., 28311549, 28311550, 28311551]]),),\n",
       " (tensor([[28311552, 28311553, 28311554,  ..., 28312317, 28312318, 28312319],\n",
       "          [28312320, 28312321, 28312322,  ..., 28313085, 28313086, 28313087],\n",
       "          [28313088, 28313089, 28313090,  ..., 28313853, 28313854, 28313855],\n",
       "          ...,\n",
       "          [28702464, 28702465, 28702466,  ..., 28703229, 28703230, 28703231],\n",
       "          [28703232, 28703233, 28703234,  ..., 28703997, 28703998, 28703999],\n",
       "          [28704000, 28704001, 28704002,  ..., 28704765, 28704766, 28704767]]),),\n",
       " (tensor([[28704768, 28704769, 28704770,  ..., 28705533, 28705534, 28705535],\n",
       "          [28705536, 28705537, 28705538,  ..., 28706301, 28706302, 28706303],\n",
       "          [28706304, 28706305, 28706306,  ..., 28707069, 28707070, 28707071],\n",
       "          ...,\n",
       "          [29095680, 29095681, 29095682,  ..., 29096445, 29096446, 29096447],\n",
       "          [29096448, 29096449, 29096450,  ..., 29097213, 29097214, 29097215],\n",
       "          [29097216, 29097217, 29097218,  ..., 29097981, 29097982, 29097983]]),),\n",
       " (tensor([[29097984, 29097985, 29097986,  ..., 29098749, 29098750, 29098751],\n",
       "          [29098752, 29098753, 29098754,  ..., 29099517, 29099518, 29099519],\n",
       "          [29099520, 29099521, 29099522,  ..., 29100285, 29100286, 29100287],\n",
       "          ...,\n",
       "          [29488896, 29488897, 29488898,  ..., 29489661, 29489662, 29489663],\n",
       "          [29489664, 29489665, 29489666,  ..., 29490429, 29490430, 29490431],\n",
       "          [29490432, 29490433, 29490434,  ..., 29491197, 29491198, 29491199]]),),\n",
       " (tensor([[29491200, 29491201, 29491202,  ..., 29491965, 29491966, 29491967],\n",
       "          [29491968, 29491969, 29491970,  ..., 29492733, 29492734, 29492735],\n",
       "          [29492736, 29492737, 29492738,  ..., 29493501, 29493502, 29493503],\n",
       "          ...,\n",
       "          [29882112, 29882113, 29882114,  ..., 29882877, 29882878, 29882879],\n",
       "          [29882880, 29882881, 29882882,  ..., 29883645, 29883646, 29883647],\n",
       "          [29883648, 29883649, 29883650,  ..., 29884413, 29884414, 29884415]]),),\n",
       " (tensor([[29884416, 29884417, 29884418,  ..., 29885181, 29885182, 29885183],\n",
       "          [29885184, 29885185, 29885186,  ..., 29885949, 29885950, 29885951],\n",
       "          [29885952, 29885953, 29885954,  ..., 29886717, 29886718, 29886719],\n",
       "          ...,\n",
       "          [30275328, 30275329, 30275330,  ..., 30276093, 30276094, 30276095],\n",
       "          [30276096, 30276097, 30276098,  ..., 30276861, 30276862, 30276863],\n",
       "          [30276864, 30276865, 30276866,  ..., 30277629, 30277630, 30277631]]),),\n",
       " (tensor([[30277632, 30277633, 30277634,  ..., 30278397, 30278398, 30278399],\n",
       "          [30278400, 30278401, 30278402,  ..., 30279165, 30279166, 30279167],\n",
       "          [30279168, 30279169, 30279170,  ..., 30279933, 30279934, 30279935],\n",
       "          ...,\n",
       "          [30668544, 30668545, 30668546,  ..., 30669309, 30669310, 30669311],\n",
       "          [30669312, 30669313, 30669314,  ..., 30670077, 30670078, 30670079],\n",
       "          [30670080, 30670081, 30670082,  ..., 30670845, 30670846, 30670847]]),),\n",
       " (tensor([[30670848, 30670849, 30670850,  ..., 30671613, 30671614, 30671615],\n",
       "          [30671616, 30671617, 30671618,  ..., 30672381, 30672382, 30672383],\n",
       "          [30672384, 30672385, 30672386,  ..., 30673149, 30673150, 30673151],\n",
       "          ...,\n",
       "          [31061760, 31061761, 31061762,  ..., 31062525, 31062526, 31062527],\n",
       "          [31062528, 31062529, 31062530,  ..., 31063293, 31063294, 31063295],\n",
       "          [31063296, 31063297, 31063298,  ..., 31064061, 31064062, 31064063]]),),\n",
       " (tensor([[31064064, 31064065, 31064066,  ..., 31064829, 31064830, 31064831],\n",
       "          [31064832, 31064833, 31064834,  ..., 31065597, 31065598, 31065599],\n",
       "          [31065600, 31065601, 31065602,  ..., 31066365, 31066366, 31066367],\n",
       "          ...,\n",
       "          [31454976, 31454977, 31454978,  ..., 31455741, 31455742, 31455743],\n",
       "          [31455744, 31455745, 31455746,  ..., 31456509, 31456510, 31456511],\n",
       "          [31456512, 31456513, 31456514,  ..., 31457277, 31457278, 31457279]]),),\n",
       " (tensor([[31457280, 31457281, 31457282,  ..., 31458045, 31458046, 31458047],\n",
       "          [31458048, 31458049, 31458050,  ..., 31458813, 31458814, 31458815],\n",
       "          [31458816, 31458817, 31458818,  ..., 31459581, 31459582, 31459583],\n",
       "          ...,\n",
       "          [31848192, 31848193, 31848194,  ..., 31848957, 31848958, 31848959],\n",
       "          [31848960, 31848961, 31848962,  ..., 31849725, 31849726, 31849727],\n",
       "          [31849728, 31849729, 31849730,  ..., 31850493, 31850494, 31850495]]),),\n",
       " (tensor([[31850496, 31850497, 31850498,  ..., 31851261, 31851262, 31851263],\n",
       "          [31851264, 31851265, 31851266,  ..., 31852029, 31852030, 31852031],\n",
       "          [31852032, 31852033, 31852034,  ..., 31852797, 31852798, 31852799],\n",
       "          ...,\n",
       "          [32241408, 32241409, 32241410,  ..., 32242173, 32242174, 32242175],\n",
       "          [32242176, 32242177, 32242178,  ..., 32242941, 32242942, 32242943],\n",
       "          [32242944, 32242945, 32242946,  ..., 32243709, 32243710, 32243711]]),),\n",
       " (tensor([[32243712, 32243713, 32243714,  ..., 32244477, 32244478, 32244479],\n",
       "          [32244480, 32244481, 32244482,  ..., 32245245, 32245246, 32245247],\n",
       "          [32245248, 32245249, 32245250,  ..., 32246013, 32246014, 32246015],\n",
       "          ...,\n",
       "          [32634624, 32634625, 32634626,  ..., 32635389, 32635390, 32635391],\n",
       "          [32635392, 32635393, 32635394,  ..., 32636157, 32636158, 32636159],\n",
       "          [32636160, 32636161, 32636162,  ..., 32636925, 32636926, 32636927]]),),\n",
       " (tensor([[32636928, 32636929, 32636930,  ..., 32637693, 32637694, 32637695],\n",
       "          [32637696, 32637697, 32637698,  ..., 32638461, 32638462, 32638463],\n",
       "          [32638464, 32638465, 32638466,  ..., 32639229, 32639230, 32639231],\n",
       "          ...,\n",
       "          [33027840, 33027841, 33027842,  ..., 33028605, 33028606, 33028607],\n",
       "          [33028608, 33028609, 33028610,  ..., 33029373, 33029374, 33029375],\n",
       "          [33029376, 33029377, 33029378,  ..., 33030141, 33030142, 33030143]]),),\n",
       " (tensor([[33030144, 33030145, 33030146,  ..., 33030909, 33030910, 33030911],\n",
       "          [33030912, 33030913, 33030914,  ..., 33031677, 33031678, 33031679],\n",
       "          [33031680, 33031681, 33031682,  ..., 33032445, 33032446, 33032447],\n",
       "          ...,\n",
       "          [33421056, 33421057, 33421058,  ..., 33421821, 33421822, 33421823],\n",
       "          [33421824, 33421825, 33421826,  ..., 33422589, 33422590, 33422591],\n",
       "          [33422592, 33422593, 33422594,  ..., 33423357, 33423358, 33423359]]),),\n",
       " (tensor([[33423360, 33423361, 33423362,  ..., 33424125, 33424126, 33424127],\n",
       "          [33424128, 33424129, 33424130,  ..., 33424893, 33424894, 33424895],\n",
       "          [33424896, 33424897, 33424898,  ..., 33425661, 33425662, 33425663],\n",
       "          ...,\n",
       "          [33814272, 33814273, 33814274,  ..., 33815037, 33815038, 33815039],\n",
       "          [33815040, 33815041, 33815042,  ..., 33815805, 33815806, 33815807],\n",
       "          [33815808, 33815809, 33815810,  ..., 33816573, 33816574, 33816575]]),),\n",
       " (tensor([[33816576, 33816577, 33816578,  ..., 33817341, 33817342, 33817343],\n",
       "          [33817344, 33817345, 33817346,  ..., 33818109, 33818110, 33818111],\n",
       "          [33818112, 33818113, 33818114,  ..., 33818877, 33818878, 33818879],\n",
       "          ...,\n",
       "          [34207488, 34207489, 34207490,  ..., 34208253, 34208254, 34208255],\n",
       "          [34208256, 34208257, 34208258,  ..., 34209021, 34209022, 34209023],\n",
       "          [34209024, 34209025, 34209026,  ..., 34209789, 34209790, 34209791]]),),\n",
       " (tensor([[34209792, 34209793, 34209794,  ..., 34210557, 34210558, 34210559],\n",
       "          [34210560, 34210561, 34210562,  ..., 34211325, 34211326, 34211327],\n",
       "          [34211328, 34211329, 34211330,  ..., 34212093, 34212094, 34212095],\n",
       "          ...,\n",
       "          [34600704, 34600705, 34600706,  ..., 34601469, 34601470, 34601471],\n",
       "          [34601472, 34601473, 34601474,  ..., 34602237, 34602238, 34602239],\n",
       "          [34602240, 34602241, 34602242,  ..., 34603005, 34603006, 34603007]]),),\n",
       " (tensor([[34603008, 34603009, 34603010,  ..., 34603773, 34603774, 34603775],\n",
       "          [34603776, 34603777, 34603778,  ..., 34604541, 34604542, 34604543],\n",
       "          [34604544, 34604545, 34604546,  ..., 34605309, 34605310, 34605311],\n",
       "          ...,\n",
       "          [34993920, 34993921, 34993922,  ..., 34994685, 34994686, 34994687],\n",
       "          [34994688, 34994689, 34994690,  ..., 34995453, 34995454, 34995455],\n",
       "          [34995456, 34995457, 34995458,  ..., 34996221, 34996222, 34996223]]),)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.concat((torch.LongTensor([[1,2,3], [1,2,3]]), torch.LongTensor([[1,2,3], [1,2,3]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
